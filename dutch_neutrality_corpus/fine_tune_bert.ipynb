{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Downloading and preparing dataset dutch_neutrality_corpus_dataset/sample (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /Users/nicholasmartin/.cache/huggingface/datasets/dutch_neutrality_corpus_dataset/sample/1.0.0/16587180dddf2cab76bf5b579914f85f38a749ef6d34c71a87a06a418b8d90dd...\n0 examples [00:00, ? examples/s]data_dir:  {'train': '/Users/nicholasmartin/.cache/huggingface/datasets/downloads/38c090853a9af4b7f11dc8b1369f03f0cdbb7941a89916a40968bfdd0de0a8e4', 'validation': '/Users/nicholasmartin/.cache/huggingface/datasets/downloads/66aa9224ada692c0b70f87f8a5b28261123d5759f6e5e504dbc08be8748c0c2d', 'test': '/Users/nicholasmartin/.cache/huggingface/datasets/downloads/6294e07d5a68b2f68aada58f113cb5bc726eeced462b15f9ee7caf994dec0740'}\n0 examples [00:00, ? examples/s]Dataset dutch_neutrality_corpus_dataset downloaded and prepared to /Users/nicholasmartin/.cache/huggingface/datasets/dutch_neutrality_corpus_dataset/sample/1.0.0/16587180dddf2cab76bf5b579914f85f38a749ef6d34c71a87a06a418b8d90dd. Subsequent calls will reuse this data.\n"
    }
   ],
   "source": [
    "datasets = load_dataset('dataset_loader.py', 'sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'tokens', 'class_labels'],\n        num_rows: 1212\n    })\n    validation: Dataset({\n        features: ['text', 'tokens', 'class_labels'],\n        num_rows: 260\n    })\n    test: Dataset({\n        features: ['text', 'tokens', 'class_labels'],\n        num_rows: 260\n    })\n})"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[\"train\"].features[f\"ner_tags\"]\n",
    "label_list = datasets[\"train\"].features[f\"class_labels\"].feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['SUBJ', 'NEUT']"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'input_ids': [101, 7632, 3501, 2003, 8254, 5104, 2807, 25312, 6593, 2666, 6767, 2953, 5831, 12079, 3158, 2139, 3769, 15859, 29472, 2818, 6971, 2063, 6102, 13719, 2112, 28418, 11867, 1999, 2139, 26922, 2063, 27829, 2121, 4372, 8254, 5104, 2997, 2455, 20806, 15992, 2112, 28418, 6767, 2953, 5831, 12079, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "tokenizer(\n",
    "    datasets[\"train\"][0]['tokens'],\n",
    "    is_split_into_words=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['[CLS]', 'de', 'tr', '##adi', '##ties', 'en', 'de', 'japan', '##se', 'same', '##nl', '##eving', 'vera', '##nder', '##en', 'ni', '##et', 's', '##nel', '[SEP]']\n"
    }
   ],
   "source": [
    "example = datasets[\"train\"][4]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[None, 0, 1, 1, 1, 2, 3, 4, 4, 5, 5, 5, 6, 6, 6, 7, 7, 8, 8, None]\n"
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "20 20\n"
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[\"class_labels\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"class_labels\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_datasets['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 2/2 [00:00<00:00,  3.77ba/s]\n"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    \"experiments\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'EUT': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n 'overall_precision': 1.0,\n 'overall_recall': 1.0,\n 'overall_f1': 1.0,\n 'overall_accuracy': 1.0}"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[\"class_labels\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"train\"],  # Change to 'validation'...\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 109/109 [14:30<00:00,  4.66s/it]\n\n\n  0%|          | 0/109 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n\n\n  2%|▏         | 2/109 [00:01<01:16,  1.39it/s]\u001b[A\u001b[A\u001b[A\n\n\n  3%|▎         | 3/109 [00:05<02:47,  1.58s/it]\u001b[A\u001b[A\u001b[A\n\n\n  4%|▎         | 4/109 [00:07<03:22,  1.93s/it]\u001b[A\u001b[A\u001b[A\n\n\n  5%|▍         | 5/109 [00:08<02:53,  1.67s/it]\u001b[A\u001b[A\u001b[A\n\n\n  6%|▌         | 6/109 [00:09<02:33,  1.49s/it]\u001b[A\u001b[A\u001b[A\n\n\n  6%|▋         | 7/109 [00:11<02:32,  1.49s/it]\u001b[A\u001b[A\u001b[A\n\n\n  7%|▋         | 8/109 [00:13<02:58,  1.76s/it]\u001b[A\u001b[A\u001b[A\n\n\n  8%|▊         | 9/109 [00:17<03:43,  2.24s/it]\u001b[A\u001b[A\u001b[A\n\n\n  9%|▉         | 10/109 [00:18<03:25,  2.07s/it]\u001b[A\u001b[A\u001b[A\n\n\n 10%|█         | 11/109 [00:20<03:15,  2.00s/it]\u001b[A\u001b[A\u001b[A\n\n\n 11%|█         | 12/109 [00:22<03:09,  1.96s/it]\u001b[A\u001b[A\u001b[A\n\n\n 12%|█▏        | 13/109 [00:23<02:38,  1.65s/it]\u001b[A\u001b[A\u001b[A\n\n\n 13%|█▎        | 14/109 [00:26<03:13,  2.04s/it]\u001b[A\u001b[A\u001b[A\n\n\n 14%|█▍        | 15/109 [00:27<02:49,  1.80s/it]\u001b[A\u001b[A\u001b[A\n\n\n 15%|█▍        | 16/109 [00:28<02:28,  1.60s/it]\u001b[A\u001b[A\u001b[A\n\n\n 16%|█▌        | 17/109 [00:31<03:06,  2.02s/it]\u001b[A\u001b[A\u001b[A\n\n\n 17%|█▋        | 18/109 [00:33<02:52,  1.89s/it]\u001b[A\u001b[A\u001b[A\n\n\n 17%|█▋        | 19/109 [00:35<03:01,  2.01s/it]\u001b[A\u001b[A\u001b[A\n\n\n 18%|█▊        | 20/109 [00:37<02:52,  1.94s/it]\u001b[A\u001b[A\u001b[A\n\n\n 19%|█▉        | 21/109 [00:39<02:55,  2.00s/it]\u001b[A\u001b[A\u001b[A\n\n\n 20%|██        | 22/109 [00:41<02:40,  1.84s/it]\u001b[A\u001b[A\u001b[A\n\n\n 21%|██        | 23/109 [00:42<02:17,  1.59s/it]\u001b[A\u001b[A\u001b[A\n\n\n 22%|██▏       | 24/109 [00:43<02:16,  1.61s/it]\u001b[A\u001b[A\u001b[A\n\n\n 23%|██▎       | 25/109 [00:45<02:27,  1.76s/it]\u001b[A\u001b[A\u001b[A\n\n\n 24%|██▍       | 26/109 [00:47<02:30,  1.82s/it]\u001b[A\u001b[A\u001b[A\n\n\n 25%|██▍       | 27/109 [00:49<02:27,  1.80s/it]\u001b[A\u001b[A\u001b[A\n\n\n 26%|██▌       | 28/109 [00:51<02:20,  1.74s/it]\u001b[A\u001b[A\u001b[A\n\n\n 27%|██▋       | 29/109 [00:52<02:01,  1.52s/it]\u001b[A\u001b[A\u001b[A\n\n\n 28%|██▊       | 30/109 [00:53<01:55,  1.46s/it]\u001b[A\u001b[A\u001b[A\n\n\n 28%|██▊       | 31/109 [00:55<02:02,  1.57s/it]\u001b[A\u001b[A\u001b[A\n\n\n 29%|██▉       | 32/109 [00:56<01:51,  1.45s/it]\u001b[A\u001b[A\u001b[A\n\n\n 30%|███       | 33/109 [00:58<01:55,  1.52s/it]\u001b[A\u001b[A\u001b[A\n\n\n 31%|███       | 34/109 [01:00<02:12,  1.76s/it]\u001b[A\u001b[A\u001b[A\n\n\n 32%|███▏      | 35/109 [01:02<02:20,  1.90s/it]\u001b[A\u001b[A\u001b[A\n\n\n 33%|███▎      | 36/109 [01:04<02:11,  1.80s/it]\u001b[A\u001b[A\u001b[A\n\n\n 34%|███▍      | 37/109 [01:06<02:09,  1.79s/it]\u001b[A\u001b[A\u001b[A\n\n\n 35%|███▍      | 38/109 [01:09<02:38,  2.23s/it]\u001b[A\u001b[A\u001b[A\n\n\n 36%|███▌      | 39/109 [01:10<02:16,  1.95s/it]\u001b[A\u001b[A\u001b[A\n\n\n 37%|███▋      | 40/109 [01:12<02:07,  1.85s/it]\u001b[A\u001b[A\u001b[A\n\n\n 38%|███▊      | 41/109 [01:13<01:58,  1.74s/it]\u001b[A\u001b[A\u001b[A\n\n\n 39%|███▊      | 42/109 [01:15<01:56,  1.75s/it]\u001b[A\u001b[A\u001b[A\n\n\n 39%|███▉      | 43/109 [01:16<01:48,  1.65s/it]\u001b[A\u001b[A\u001b[A\n\n\n 40%|████      | 44/109 [01:19<02:04,  1.92s/it]\u001b[A\u001b[A\u001b[A\n\n\n 41%|████▏     | 45/109 [01:20<01:46,  1.66s/it]\u001b[A\u001b[A\u001b[A\n\n\n 42%|████▏     | 46/109 [01:22<01:52,  1.79s/it]\u001b[A\u001b[A\u001b[A\n\n\n 43%|████▎     | 47/109 [01:23<01:43,  1.66s/it]\u001b[A\u001b[A\u001b[A\n\n\n 44%|████▍     | 48/109 [01:25<01:39,  1.62s/it]\u001b[A\u001b[A\u001b[A\n\n\n 45%|████▍     | 49/109 [01:26<01:36,  1.60s/it]\u001b[A\u001b[A\u001b[A\n\n\n 46%|████▌     | 50/109 [01:27<01:23,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\n 47%|████▋     | 51/109 [01:29<01:27,  1.51s/it]\u001b[A\u001b[A\u001b[A\n\n\n 48%|████▊     | 52/109 [01:32<01:47,  1.88s/it]\u001b[A\u001b[A\u001b[A\n\n\n 49%|████▊     | 53/109 [01:33<01:28,  1.58s/it]\u001b[A\u001b[A\u001b[A\n\n\n 50%|████▉     | 54/109 [01:35<01:32,  1.68s/it]\u001b[A\u001b[A\u001b[A\n\n\n 50%|█████     | 55/109 [01:36<01:27,  1.62s/it]\u001b[A\u001b[A\u001b[A\n\n\n 51%|█████▏    | 56/109 [01:38<01:26,  1.64s/it]\u001b[A\u001b[A\u001b[A\n\n\n 52%|█████▏    | 57/109 [01:40<01:30,  1.74s/it]\u001b[A\u001b[A\u001b[A\n\n\n 53%|█████▎    | 58/109 [01:41<01:24,  1.66s/it]\u001b[A\u001b[A\u001b[A\n\n\n 54%|█████▍    | 59/109 [01:43<01:17,  1.55s/it]\u001b[A\u001b[A\u001b[A\n\n\n 55%|█████▌    | 60/109 [01:44<01:10,  1.45s/it]\u001b[A\u001b[A\u001b[A\n\n\n 56%|█████▌    | 61/109 [01:46<01:13,  1.54s/it]\u001b[A\u001b[A\u001b[A\n\n\n 57%|█████▋    | 62/109 [01:48<01:30,  1.93s/it]\u001b[A\u001b[A\u001b[A\n\n\n 58%|█████▊    | 63/109 [01:50<01:29,  1.94s/it]\u001b[A\u001b[A\u001b[A\n\n\n 59%|█████▊    | 64/109 [01:53<01:39,  2.22s/it]\u001b[A\u001b[A\u001b[A\n\n\n 60%|█████▉    | 65/109 [01:55<01:32,  2.11s/it]\u001b[A\u001b[A\u001b[A\n\n\n 61%|██████    | 66/109 [01:58<01:36,  2.23s/it]\u001b[A\u001b[A\u001b[A\n\n\n 61%|██████▏   | 67/109 [01:59<01:21,  1.93s/it]\u001b[A\u001b[A\u001b[A\n\n\n 62%|██████▏   | 68/109 [02:00<01:10,  1.71s/it]\u001b[A\u001b[A\u001b[A\n\n\n 63%|██████▎   | 69/109 [02:02<01:14,  1.86s/it]\u001b[A\u001b[A\u001b[A\n\n\n 64%|██████▍   | 70/109 [02:05<01:17,  1.98s/it]\u001b[A\u001b[A\u001b[A\n\n\n 65%|██████▌   | 71/109 [02:07<01:24,  2.23s/it]\u001b[A\u001b[A\u001b[A\n\n\n 66%|██████▌   | 72/109 [02:09<01:15,  2.03s/it]\u001b[A\u001b[A\u001b[A\n\n\n 67%|██████▋   | 73/109 [02:10<01:05,  1.83s/it]\u001b[A\u001b[A\u001b[A\n\n\n 68%|██████▊   | 74/109 [02:12<01:02,  1.78s/it]\u001b[A\u001b[A\u001b[A\n\n\n 69%|██████▉   | 75/109 [02:14<01:06,  1.97s/it]\u001b[A\u001b[A\u001b[A\n\n\n 70%|██████▉   | 76/109 [02:16<01:01,  1.87s/it]\u001b[A\u001b[A\u001b[A\n\n\n 71%|███████   | 77/109 [02:17<00:50,  1.58s/it]\u001b[A\u001b[A\u001b[A\n\n\n 72%|███████▏  | 78/109 [02:18<00:47,  1.52s/it]\u001b[A\u001b[A\u001b[A\n\n\n 72%|███████▏  | 79/109 [02:19<00:41,  1.40s/it]\u001b[A\u001b[A\u001b[A\n\n\n 73%|███████▎  | 80/109 [02:21<00:42,  1.47s/it]\u001b[A\u001b[A\u001b[A\n\n\n 74%|███████▍  | 81/109 [02:22<00:39,  1.41s/it]\u001b[A\u001b[A\u001b[A\n\n\n 75%|███████▌  | 82/109 [02:24<00:40,  1.50s/it]\u001b[A\u001b[A\u001b[A\n\n\n 76%|███████▌  | 83/109 [02:27<00:49,  1.91s/it]\u001b[A\u001b[A\u001b[A\n\n\n 77%|███████▋  | 84/109 [02:28<00:44,  1.78s/it]\u001b[A\u001b[A\u001b[A\n\n\n 78%|███████▊  | 85/109 [02:30<00:41,  1.74s/it]\u001b[A\u001b[A\u001b[A\n\n\n 79%|███████▉  | 86/109 [02:31<00:37,  1.63s/it]\u001b[A\u001b[A\u001b[A\n\n\n 80%|███████▉  | 87/109 [02:33<00:35,  1.64s/it]\u001b[A\u001b[A\u001b[A\n\n\n 81%|████████  | 88/109 [02:34<00:30,  1.47s/it]\u001b[A\u001b[A\u001b[A\n\n\n 82%|████████▏ | 89/109 [02:35<00:26,  1.33s/it]\u001b[A\u001b[A\u001b[A\n\n\n 83%|████████▎ | 90/109 [02:37<00:26,  1.40s/it]\u001b[A\u001b[A\u001b[A\n\n\n 83%|████████▎ | 91/109 [02:38<00:23,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\n 84%|████████▍ | 92/109 [02:39<00:20,  1.22s/it]\u001b[A\u001b[A\u001b[A\n\n\n 85%|████████▌ | 93/109 [02:40<00:19,  1.23s/it]\u001b[A\u001b[A\u001b[A\n\n\n 86%|████████▌ | 94/109 [02:41<00:17,  1.19s/it]\u001b[A\u001b[A\u001b[A\n\n\n 87%|████████▋ | 95/109 [02:43<00:18,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\n 88%|████████▊ | 96/109 [02:45<00:19,  1.52s/it]\u001b[A\u001b[A\u001b[A\n\n\n 89%|████████▉ | 97/109 [02:46<00:17,  1.49s/it]\u001b[A\u001b[A\u001b[A\n\n\n 90%|████████▉ | 98/109 [02:48<00:15,  1.45s/it]\u001b[A\u001b[A\u001b[A\n\n\n 91%|█████████ | 99/109 [02:49<00:14,  1.42s/it]\u001b[A\u001b[A\u001b[A\n\n\n 92%|█████████▏| 100/109 [02:50<00:12,  1.40s/it]\u001b[A\u001b[A\u001b[A\n\n\n 93%|█████████▎| 101/109 [02:52<00:11,  1.38s/it]\u001b[A\u001b[A\u001b[A\n\n\n 94%|█████████▎| 102/109 [02:53<00:09,  1.37s/it]\u001b[A\u001b[A\u001b[A\n\n\n 94%|█████████▍| 103/109 [02:54<00:07,  1.30s/it]\u001b[A\u001b[A\u001b[A\n\n\n 95%|█████████▌| 104/109 [02:55<00:06,  1.26s/it]\u001b[A\u001b[A\u001b[A\n\n\n 96%|█████████▋| 105/109 [02:57<00:05,  1.32s/it]\u001b[A\u001b[A\u001b[A\n\n\n 97%|█████████▋| 106/109 [02:58<00:03,  1.19s/it]\u001b[A\u001b[A\u001b[A\n\n\n 98%|█████████▊| 107/109 [03:00<00:03,  1.65s/it]\u001b[A\u001b[A\u001b[A\n\n\n 99%|█████████▉| 108/109 [03:04<00:02,  2.14s/it]\u001b[A\u001b[A\u001b[A\n\n\n100%|██████████| 109/109 [03:04<00:00,  1.62s/it]\u001b[A\u001b[A\u001b[A\n\n\u001b[A\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\u001b[A\n\n100%|██████████| 204/204 [2:20:53<00:00,  1.68it/s]\u001b[A\u001b[A\n\n\n100%|██████████| 109/109 [17:41<00:00,  4.66s/it]\n  8%|▊         | 72/878 [2:59:44<2:39:06, 11.84s/it]\u001b[A\n\n\n\u001b[A\u001b[A\u001b[A\n\n\u001b[A\u001b[A\n\u001b[A\n\n100%|██████████| 109/109 [17:41<00:00,  4.66s/it]\n100%|██████████| 109/109 [17:41<00:00,  9.74s/it]{'eval_loss': 0.48548054695129395, 'eval_precision': 0.49191685912240185, 'eval_recall': 0.05343367826904986, 'eval_f1': 0.09639644736097754, 'eval_accuracy': 0.8085934754803435, 'eval_runtime': 190.524, 'eval_samples_per_second': 9.091, 'epoch': 1.0}\n{'train_runtime': 1061.3375, 'train_samples_per_second': 0.103, 'epoch': 1.0}\n\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=109, training_loss=0.49466771816988603, metrics={'train_runtime': 1061.3375, 'train_samples_per_second': 0.103, 'epoch': 1.0})"
     },
     "metadata": {},
     "execution_count": 85
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 109/109 [02:52<00:00,  1.58s/it]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'eval_loss': 0.48548054695129395,\n 'eval_precision': 0.49191685912240185,\n 'eval_recall': 0.05343367826904986,\n 'eval_f1': 0.09639644736097754,\n 'eval_accuracy': 0.8085934754803435,\n 'eval_runtime': 173.2819,\n 'eval_samples_per_second': 9.995,\n 'epoch': 1.0}"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 109/109 [02:45<00:00,  1.61s/it]"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'EUT': {'precision': 0.49191685912240185,\n  'recall': 0.36613665663944994,\n  'f1': 0.4198078344419808,\n  'number': 2327},\n 'UBJ': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 13618},\n 'overall_precision': 0.49191685912240185,\n 'overall_recall': 0.05343367826904986,\n 'overall_f1': 0.09639644736097754,\n 'overall_accuracy': 0.8085934754803435}"
     },
     "metadata": {},
     "execution_count": 88
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"train\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(\n",
    "    predictions=true_predictions, \n",
    "    references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1613406243420",
   "display_name": "Python 3.7.3 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}